{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive RAG Dataset Builder - Kaggle GPU Version\n",
    "\n",
    "This notebook builds domain relevance and hallucination risk datasets using weak supervision labeling techniques.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Upload preprocessed data files to Kaggle dataset input\n",
    "- Upload `ai_ml_lexicon.txt` to Kaggle dataset input\n",
    "- Enable GPU acceleration in Kaggle settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:48:58.067720Z",
     "iopub.status.busy": "2025-08-14T08:48:58.067258Z",
     "iopub.status.idle": "2025-08-14T08:51:06.663967Z",
     "shell.execute_reply": "2025-08-14T08:51:06.659921Z",
     "shell.execute_reply.started": "2025-08-14T08:48:58.067687Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers sentence-transformers accelerate tokenizers rank-bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:51:19.652564Z",
     "iopub.status.busy": "2025-08-14T08:51:19.651982Z",
     "iopub.status.idle": "2025-08-14T08:52:05.407270Z",
     "shell.execute_reply": "2025-08-14T08:52:05.405607Z",
     "shell.execute_reply.started": "2025-08-14T08:51:19.652497Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import re\n",
    "from rank_bm25 import BM25Okapi\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from transformers import pipeline\n",
    "import math\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:52:11.810222Z",
     "iopub.status.busy": "2025-08-14T08:52:11.809112Z",
     "iopub.status.idle": "2025-08-14T08:52:11.824376Z",
     "shell.execute_reply": "2025-08-14T08:52:11.822989Z",
     "shell.execute_reply.started": "2025-08-14T08:52:11.810178Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Kaggle paths configuration\n",
    "CONFIG = {\n",
    "    \"paths\": {\n",
    "        \"ai_corpus\": \"/kaggle/input/arag-data/ai_ml_corpus.jsonl\",\n",
    "        \"questions\": \"/kaggle/input/arag-data/hallucination_dataset_01.json\",  # Will load all JSON files from this directory\n",
    "        \"output_dir\": \"/kaggle/working/processed\",\n",
    "        \"artifacts_dir\": \"/kaggle/working/artifacts\",\n",
    "        \"lexicon_path\": \"/kaggle/input/arag-data/ai_ml_lexicon.txt\"\n",
    "    },\n",
    "    \"retrieval\": {\n",
    "        \"top_k\": 5,\n",
    "        \"bm25\": {\"use_title\": True},\n",
    "        \"vector\": {\n",
    "            \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "            \"normalize\": True,\n",
    "            \"batch_size\": 128\n",
    "        }\n",
    "    },\n",
    "    \"models\": {\n",
    "        \"cross_encoder\": \"cross-encoder/stsb-distilroberta-base\",\n",
    "        \"text_gen_model\": \"google/flan-t5-base\"\n",
    "    },\n",
    "    \"weak_labeling\": {\n",
    "        \"hallucination\": {\n",
    "            \"thresholds\": {\n",
    "                \"factual_precision_risk_high_risk\": 0.3,\n",
    "                \"obscurity_risk_high_risk\": 0.3,\n",
    "                \"complexity_risk_high_risk\": 0.3,\n",
    "                \"answer_deviation_high_risk\": 0.7\n",
    "            }\n",
    "        },\n",
    "        \"domain\": {\n",
    "            \"thresholds\": {\n",
    "                \"lexicon_high\": 0.01,\n",
    "                \"embedding_sim_high\": 0.25,\n",
    "                \"bm25_ratio_high\": 40,\n",
    "                \"cross_encoder_high\": 0.3\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"aggregation\": {\"seed\": 42}\n",
    "}\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(CONFIG[\"paths\"][\"output_dir\"], exist_ok=True)\n",
    "os.makedirs(CONFIG[\"paths\"][\"artifacts_dir\"], exist_ok=True)\n",
    "\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Structures and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:52:25.898935Z",
     "iopub.status.busy": "2025-08-14T08:52:25.898439Z",
     "iopub.status.idle": "2025-08-14T08:52:25.919935Z",
     "shell.execute_reply": "2025-08-14T08:52:25.918606Z",
     "shell.execute_reply.started": "2025-08-14T08:52:25.898905Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Example:\n",
    "    ex_id: str\n",
    "    question: str\n",
    "    answer: str | None\n",
    "    source: str\n",
    "\n",
    "@dataclass\n",
    "class Document:\n",
    "    doc_id: str\n",
    "    text: str\n",
    "    title: Optional[str] = None\n",
    "\n",
    "def load_questions(path: str) -> List[Example]:\n",
    "    \"\"\"Load questions from directory or file\"\"\"\n",
    "    items: List[Example] = []\n",
    "    \n",
    "    if os.path.isdir(path):\n",
    "        json_files = [f for f in os.listdir(path) if f.endswith('.json')]\n",
    "        print(f\"Found {len(json_files)} JSON files in {path}\")\n",
    "        \n",
    "        for json_file in json_files:\n",
    "            file_path = os.path.join(path, json_file)\n",
    "            initial_count = len(items)\n",
    "            \n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    for obj in data:\n",
    "                        items.append(Example(\n",
    "                            ex_id=str(obj.get(\"id\")), \n",
    "                            question=obj.get(\"question\"), \n",
    "                            answer=obj.get(\"answer\"), \n",
    "                            source=str(obj.get(\"source\"))\n",
    "                        ))\n",
    "                else:\n",
    "                    items.append(Example(\n",
    "                        ex_id=str(data.get(\"id\")), \n",
    "                        question=data.get(\"question\"), \n",
    "                        answer=data.get(\"answer\"), \n",
    "                        source=str(data.get(\"source\"))\n",
    "                    ))\n",
    "            \n",
    "            questions_loaded = len(items) - initial_count\n",
    "            print(f\"Loaded {questions_loaded} questions from {json_file}\")\n",
    "    \n",
    "    elif os.path.isfile(path):\n",
    "        if path.endswith('.jsonl'):\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    obj = json.loads(line)\n",
    "                    items.append(Example(\n",
    "                        ex_id=str(obj.get(\"id\")), \n",
    "                        question=obj.get(\"question\"), \n",
    "                        answer=obj.get(\"answer\"), \n",
    "                        source=str(obj.get(\"source\"))\n",
    "                    ))\n",
    "        else:\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    for obj in data:\n",
    "                        items.append(Example(\n",
    "                            ex_id=str(obj.get(\"id\")), \n",
    "                            question=obj.get(\"question\"), \n",
    "                            answer=obj.get(\"answer\"), \n",
    "                            source=str(obj.get(\"source\"))\n",
    "                        ))\n",
    "                else:\n",
    "                    items.append(Example(\n",
    "                        ex_id=str(data.get(\"id\")), \n",
    "                        question=data.get(\"question\"), \n",
    "                        answer=data.get(\"answer\"), \n",
    "                        source=str(data.get(\"source\"))\n",
    "                    ))\n",
    "    \n",
    "    return items\n",
    "\n",
    "def load_corpus(corpus_path: str) -> List[Document]:\n",
    "    \"\"\"Load corpus documents\"\"\"\n",
    "    docs = []\n",
    "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f):\n",
    "            if line.strip():\n",
    "                obj = json.loads(line)\n",
    "                docs.append(Document(\n",
    "                    doc_id=obj.get(\"id\", str(line_num)),\n",
    "                    text=obj.get(\"text\"),\n",
    "                    title=obj.get(\"title\")\n",
    "                ))\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. BM25 Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:52:30.076720Z",
     "iopub.status.busy": "2025-08-14T08:52:30.076311Z",
     "iopub.status.idle": "2025-08-14T08:52:30.090039Z",
     "shell.execute_reply": "2025-08-14T08:52:30.087819Z",
     "shell.execute_reply.started": "2025-08-14T08:52:30.076693Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def simple_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Simple tokenization\"\"\"\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n",
    "    return text.split()\n",
    "\n",
    "class BM25Index:\n",
    "    def __init__(self, docs: List[Document], use_title: bool = True):\n",
    "        self.docs = docs\n",
    "        self.use_title = use_title\n",
    "        \n",
    "        # Prepare texts for BM25\n",
    "        texts = []\n",
    "        for doc in docs:\n",
    "            if use_title and doc.title:\n",
    "                combined_text = f\"{doc.title} {doc.text}\"\n",
    "            else:\n",
    "                combined_text = doc.text\n",
    "            texts.append(combined_text)\n",
    "        \n",
    "        # Tokenize\n",
    "        tokenized_docs = [simple_tokenize(text) for text in texts]\n",
    "        \n",
    "        # Build BM25 index\n",
    "        self.bm25 = BM25Okapi(tokenized_docs)\n",
    "        print(f\"Built BM25 index with {len(docs)} documents\")\n",
    "    \n",
    "    def query(self, query: str, top_k: int = 5) -> List[Tuple[Document, float]]:\n",
    "        \"\"\"Query the BM25 index\"\"\"\n",
    "        tokenized_query = simple_tokenize(query)\n",
    "        scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Get top k\n",
    "        top_indices = np.argsort(scores)[::-1][:top_k]\n",
    "        results = [(self.docs[i], scores[i]) for i in top_indices]\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Labeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:52:59.901376Z",
     "iopub.status.busy": "2025-08-14T08:52:59.900973Z",
     "iopub.status.idle": "2025-08-14T08:52:59.934961Z",
     "shell.execute_reply": "2025-08-14T08:52:59.933675Z",
     "shell.execute_reply.started": "2025-08-14T08:52:59.901350Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DomainSignals:\n",
    "    lexicon_ratio: float\n",
    "    embedding_sim: float\n",
    "    bm25_ratio: float\n",
    "    cross_encoder_score: float\n",
    "\n",
    "@dataclass\n",
    "class HallucinationSignals:\n",
    "    factual_precision_risk: float\n",
    "    obscurity_risk: float\n",
    "    complexity_risk: float\n",
    "    #answer_deviation: float\n",
    "\n",
    "def sigmoid_score(signal: float, threshold: float, scaling: float = 5.0) -> float:\n",
    "    \"\"\"Convert signal to continuous score based on distance from threshold.\"\"\"\n",
    "    return 1.0 / (1.0 + math.exp(-(signal - threshold) * scaling))\n",
    "        \n",
    "class DomainLabelers:\n",
    "    def __init__(self, embedding_model: str, cross_encoder_model: str, lexicon: List[str]):\n",
    "        # Use GPU if available\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        self.embedding_model = SentenceTransformer(embedding_model, device=device)\n",
    "        self.cross_encoder = CrossEncoder(cross_encoder_model, device=device)\n",
    "        self.lexicon = set([term.lower() for term in lexicon])\n",
    "        self.ai_centroid = None\n",
    "        \n",
    "        print(f\"Loaded domain labelers on device: {device}\")\n",
    "    \n",
    "    def build_ai_centroid(self, ai_texts: List[str]):\n",
    "        \"\"\"Build AI domain centroid from corpus\"\"\"\n",
    "        print(f\"Building AI centroid from {len(ai_texts)} texts...\")\n",
    "        embeddings = self.embedding_model.encode(ai_texts, batch_size=32, show_progress_bar=True, normalize_embeddings=True)\n",
    "        self.ai_centroid = np.mean(embeddings, axis=0)\n",
    "        print(\"AI centroid built successfully\")\n",
    "    \n",
    "    def compute_signals(self, question: str, ai_idx: BM25Index, top_k: int = 5) -> DomainSignals:\n",
    "        # Lexicon ratio\n",
    "        hits = 0\n",
    "        for term in self.lexicon:\n",
    "            if term.lower() in question.lower():\n",
    "                hits += 1\n",
    "        lexicon_ratio = hits / max(len(self.lexicon), 1)\n",
    "        \n",
    "        # Embedding similarity\n",
    "        if self.ai_centroid is not None:\n",
    "            q_emb = self.embedding_model.encode([question], convert_to_numpy=True, show_progress_bar=False, normalize_embeddings=True)[0]\n",
    "            embedding_sim = float(np.dot(q_emb, self.ai_centroid) / \n",
    "                                (np.linalg.norm(q_emb) * np.linalg.norm(self.ai_centroid)))\n",
    "        else:\n",
    "            embedding_sim = 0.0\n",
    "        \n",
    "        # BM25 ratio\n",
    "        ai_results = ai_idx.query(question, top_k=top_k)\n",
    "        if ai_results:\n",
    "            avg_ai_score = np.mean([score for _, score in ai_results])\n",
    "            bm25_ratio = float(avg_ai_score)\n",
    "        else:\n",
    "            bm25_ratio = 0.0\n",
    "        \n",
    "        # Cross-encoder score\n",
    "        if ai_results:\n",
    "            ai_templates = [\n",
    "                \"artificial intelligence and machine learning concepts\",\n",
    "                \"data science and statistical methods\", \n",
    "                \"neural networks and deep learning\",\n",
    "                \"data analysis and predictive modeling\",\n",
    "                \"pattern recognition and deep learning algorithms\",\n",
    "                \"reinforcement learning and decision making\",\n",
    "                \"computer vision and image processing\",\n",
    "                \"natural language processing and text analysis\",\n",
    "            ]\n",
    "            pairs = [(question, template) for template in ai_templates]  # Top 3 for efficiency\n",
    "            ce_scores = self.cross_encoder.predict(pairs)\n",
    "            cross_encoder_score = float(np.max(ce_scores))\n",
    "        else:\n",
    "            cross_encoder_score = 0.0\n",
    "        \n",
    "        return DomainSignals(\n",
    "            lexicon_ratio=lexicon_ratio,\n",
    "            embedding_sim=embedding_sim,\n",
    "            bm25_ratio=bm25_ratio,\n",
    "            cross_encoder_score=cross_encoder_score\n",
    "        )\n",
    "    \n",
    "    def weak_votes(self, signals: DomainSignals, thresholds: Dict[str, float]) -> Dict[str, int]:\n",
    "        \"\"\"Generate weak supervision votes\"\"\"\n",
    "        votes: Dict[str, float] = {}\n",
    "        votes[\"lexicon\"] = sigmoid_score(signals.lexicon_ratio, thresholds.get(\"lexicon_high\", 0.01), scaling=400)\n",
    "        votes[\"embed\"] = sigmoid_score(signals.embedding_sim, thresholds.get(\"embedding_sim_high\", 0.25), scaling=4)\n",
    "        votes[\"bm25_ratio\"] = sigmoid_score(signals.bm25_ratio, thresholds.get(\"bm25_ratio_high\", 40), scaling=0.04)  # Lower scaling for wide range\n",
    "        votes[\"cross_encoder\"] = sigmoid_score(signals.cross_encoder_score, thresholds.get(\"cross_encoder_high\", 0.3), scaling=5)\n",
    "        return votes\n",
    "\n",
    "class HallucinationLabelers:\n",
    "    def __init__(self, cross_encoder: str, text_gen_model: str, embedding_model: str):\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        self.cross_encoder = CrossEncoder(cross_encoder, device=device)\n",
    "        self.text_gen = pipeline(\"text2text-generation\", model=text_gen_model, device=0 if torch.cuda.is_available() else -1)\n",
    "        #self.embedding_model = SentenceTransformer(embedding_model, device=device)\n",
    "\n",
    "        print(f\"Loaded hallucination labelers on device: {device}\")\n",
    "    \n",
    "    def compute_signals(self, question: str, answer: str) -> HallucinationSignals:\n",
    "        hypotheses = {\n",
    "                \"factual_precision\": \"This question demands exact facts, specific numbers, precise dates, or detailed information that could easily be fabricated if unknown.\",\n",
    "                \"complexity\": \"This question involves multi-step reasoning, complex analysis, or connecting multiple concepts together.\",\n",
    "                \"obscurity_high\": \"This question covers specialized, technical, or niche topics that most people wouldn't know.\",\n",
    "                \"obscurity_low\": \"This question covers common knowledge, basic facts, or widely known information.\"\n",
    "            }\n",
    "        pairs = [(question, hypothesis) for hypothesis in hypotheses.values()]\n",
    "        scores = self.cross_encoder.predict(pairs)\n",
    "        risk_scores = {}\n",
    "        for i, risk_type in enumerate(hypotheses.keys()):\n",
    "            score = float(scores[i])\n",
    "            normalized_score = max(0.0, min(1.0, score))\n",
    "            risk_scores[risk_type] = normalized_score\n",
    "            \n",
    "        # Calculate obscurity risk as ratio: obscure_similarity / (obscure + common)\n",
    "        obscure_sim = risk_scores[\"obscurity_high\"]\n",
    "        common_sim = risk_scores[\"obscurity_low\"]\n",
    "        if obscure_sim + common_sim > 0:\n",
    "            obscurity_risk = obscure_sim / (obscure_sim + common_sim)\n",
    "        else:\n",
    "            obscurity_risk = 0.4\n",
    "        \n",
    "        # Answer deviation (generate alternative and compare)\n",
    "        #generated = self.text_gen(question[:200], max_length=100, do_sample=True, temperature=0.7)\n",
    "        #generated_text = generated[0]['generated_text'].strip()\n",
    "        \n",
    "        #gen_emb = self.embedding_model.encode([generated_text])\n",
    "        #actual_emb = self.embedding_model.encode([answer])\n",
    "        \n",
    "        #similarity = float(gen_emb @ actual_emb.T)\n",
    "        #deviation = 1.0 - max(0.0, similarity)  # Higher deviation = more risk\n",
    "        \n",
    "        #answer_deviation = min(1.0, max(0.0, deviation))\n",
    "\n",
    "        return HallucinationSignals(\n",
    "            factual_precision_risk=risk_scores[\"factual_precision\"],\n",
    "            obscurity_risk=obscurity_risk,\n",
    "            complexity_risk=risk_scores[\"complexity\"],\n",
    "            #answer_deviation=answer_deviation\n",
    "        )\n",
    "    \n",
    "    def weak_votes(self, signals: HallucinationSignals, thresholds: Dict[str, float]) -> Dict[str, int]:\n",
    "        \"\"\"Generate weak supervision votes\"\"\"\n",
    "        \n",
    "        votes: Dict[str, float] = {}\n",
    "        votes[\"factual_precision\"] = sigmoid_score(signals.factual_precision_risk, thresholds.get(\"factual_precision_risk_high_risk\", 0.4), scaling=5)\n",
    "        votes[\"obscurity\"] = sigmoid_score(signals.obscurity_risk, thresholds.get(\"obscurity_risk_high_risk\", 0.4), scaling=5)\n",
    "        votes[\"complexity\"] = sigmoid_score(signals.complexity_risk, thresholds.get(\"complexity_risk_high_risk\", 0.4), scaling=5)\n",
    "        #votes[\"answer_dev\"] = sigmoid_score(signals.answer_deviation, thresholds.get(\"answer_deviation_high_risk\", 0.5), scaling=5)\n",
    "        return votes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Label Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:59:07.070691Z",
     "iopub.status.busy": "2025-08-14T08:59:07.070263Z",
     "iopub.status.idle": "2025-08-14T08:59:07.085023Z",
     "shell.execute_reply": "2025-08-14T08:59:07.083016Z",
     "shell.execute_reply.started": "2025-08-14T08:59:07.070665Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_label_matrix(votes_list: List[Dict[str, int]], labeler_names: List[str]) -> np.ndarray:\n",
    "    \"\"\"Build label matrix for Snorkel\"\"\"\n",
    "    L = np.full((len(votes_list), len(labeler_names)), float(-1), dtype=float)\n",
    "    for i, votes in enumerate(votes_list):\n",
    "        for j, name in enumerate(labeler_names):\n",
    "            if name in votes:\n",
    "                L[i, j] = float(votes[name])\n",
    "    return L\n",
    "\n",
    "def aggregate_probabilities(L: np.ndarray, seed: int = 42, labeler_names: Optional[List[str]] = None) -> np.ndarray:\n",
    "    \"\"\"Aggregate weak supervision labels using Snorkel\"\"\"\n",
    "    domain_weights = {\n",
    "        #Domain Relevance\n",
    "        \"cross_encoder\": 0.35,  # Highest weight\n",
    "        \"bm25_ratio\": 0.3,\n",
    "        \"embed\": 0.25, \n",
    "        \"lexicon\": 0.2,         # Lowest weight\n",
    "        #Hallucination\n",
    "        \"factual_precision\": 0.3,\n",
    "        \"obscurity\": 0.25,\n",
    "        \"complexity\": 0.45,\n",
    "        #\"answer_dev\": 0.05\n",
    "    }\n",
    "    default_weight = 1.0 / L.shape[1] if L.shape[1] > 0 else 1.0\n",
    "    probs = []\n",
    "    for row in L:\n",
    "        vals = [v for v in row.tolist() if v != -1]\n",
    "        if not vals:\n",
    "            probs.append(0.5)\n",
    "        else:\n",
    "            if labeler_names and len(labeler_names) == len(row):\n",
    "                # Use weighted average for domain signals\n",
    "                weighted_sum = 0.0\n",
    "                confidence_sum = 0.0\n",
    "                for i, val in enumerate(row):\n",
    "                    if val != -1:\n",
    "                        weight = domain_weights.get(labeler_names[i], default_weight)\n",
    "                        #confidence = abs(val-0.5) * 2\n",
    "                        #confidence_boost = confidence ** 3 * 4\n",
    "                        #boosted_weight = weight * (1 + confidence_boost)  # Normal boost\n",
    "                        boosted_weight = weight\n",
    "                        confidence_sum += boosted_weight\n",
    "                        weighted_sum += val * boosted_weight\n",
    "                p = float(weighted_sum / confidence_sum) if confidence_sum > 0 else 0.5\n",
    "            else:\n",
    "                # Simple average for hallucination or when no labeler names provided\n",
    "                p = float(sum(vals) / len(vals))\n",
    "            probs.append(p)\n",
    "    probs = np.clip(np.array(probs), 0.0, 1.0)\n",
    "    return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Main Dataset Building Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:53:14.434170Z",
     "iopub.status.busy": "2025-08-14T08:53:14.433744Z",
     "iopub.status.idle": "2025-08-14T08:53:17.390937Z",
     "shell.execute_reply": "2025-08-14T08:53:17.389594Z",
     "shell.execute_reply.started": "2025-08-14T08:53:14.434141Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load corpus and build BM25 index\n",
    "print(\"Loading AI corpus...\")\n",
    "ai_docs = load_corpus(CONFIG[\"paths\"][\"ai_corpus\"])\n",
    "print(f\"Loaded {len(ai_docs)} AI documents\")\n",
    "\n",
    "print(\"Building BM25 index...\")\n",
    "ai_idx = BM25Index(ai_docs, use_title=CONFIG[\"retrieval\"][\"bm25\"][\"use_title\"])\n",
    "\n",
    "# Load lexicon\n",
    "print(\"Loading lexicon...\")\n",
    "with open(CONFIG[\"paths\"][\"lexicon_path\"], \"r\", encoding=\"utf-8\") as f:\n",
    "    lexicon = [ln.strip() for ln in f if ln.strip()]\n",
    "print(f\"Loaded {len(lexicon)} lexicon terms\")\n",
    "\n",
    "# Load questions\n",
    "print(\"Loading questions...\")\n",
    "examples = load_questions(CONFIG[\"paths\"][\"questions\"])\n",
    "print(f\"Loaded {len(examples)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:53:30.085551Z",
     "iopub.status.busy": "2025-08-14T08:53:30.085130Z",
     "iopub.status.idle": "2025-08-14T08:56:07.194864Z",
     "shell.execute_reply": "2025-08-14T08:56:07.193052Z",
     "shell.execute_reply.started": "2025-08-14T08:53:30.085503Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize labelers\n",
    "print(\"Initializing labelers...\")\n",
    "\n",
    "hall = HallucinationLabelers(\n",
    "    cross_encoder=CONFIG[\"models\"][\"cross_encoder\"],\n",
    "    text_gen_model=CONFIG[\"models\"][\"text_gen_model\"],\n",
    "    embedding_model=CONFIG[\"retrieval\"][\"vector\"][\"embedding_model\"]\n",
    ")\n",
    "\n",
    "dom = DomainLabelers(\n",
    "    embedding_model=CONFIG[\"retrieval\"][\"vector\"][\"embedding_model\"],\n",
    "    cross_encoder_model=CONFIG[\"models\"][\"cross_encoder\"],\n",
    "    lexicon=lexicon\n",
    ")\n",
    "\n",
    "# Build AI centroid\n",
    "print(\"Building AI domain centroid...\")\n",
    "dom.build_ai_centroid([d.text for d in ai_docs[:5000]])  # Use first 5000 for efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:57:02.067311Z",
     "iopub.status.busy": "2025-08-14T08:57:02.066923Z",
     "iopub.status.idle": "2025-08-14T08:57:47.848708Z",
     "shell.execute_reply": "2025-08-14T08:57:47.847520Z",
     "shell.execute_reply.started": "2025-08-14T08:57:02.067286Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Compute signals and build datasets\n",
    "top_k = CONFIG[\"retrieval\"][\"top_k\"]\n",
    "rows_domain: List[Dict[str, Any]] = []\n",
    "rows_hallu: List[Dict[str, Any]] = []\n",
    "\n",
    "print(\"Computing signals for all examples...\")\n",
    "for ex in tqdm(examples, desc=\"Building signals\"):\n",
    "    # Domain relevance signals\n",
    "    dom_signals = dom.compute_signals(ex.question, ai_idx, top_k)\n",
    "    dom_votes = dom.weak_votes(dom_signals, CONFIG[\"weak_labeling\"][\"domain\"][\"thresholds\"])\n",
    "    \n",
    "    rows_domain.append({\n",
    "        \"id\": ex.ex_id,\n",
    "        \"question\": ex.question,\n",
    "        \"answer\": ex.answer,\n",
    "        \"source\": ex.source,\n",
    "        \"signals\": {\n",
    "            \"lexicon_ratio\": dom_signals.lexicon_ratio,\n",
    "            \"embedding_sim\": dom_signals.embedding_sim,\n",
    "            \"bm25_ratio\": dom_signals.bm25_ratio,\n",
    "            \"cross_encoder_score\": dom_signals.cross_encoder_score,\n",
    "        },\n",
    "        \"votes\": dom_votes,\n",
    "    })\n",
    "    \n",
    "    # Hallucination signals (only if answer exists)\n",
    "    if ex.answer:\n",
    "        hall_signals = hall.compute_signals(ex.question, ex.answer)\n",
    "        hall_votes = hall.weak_votes(hall_signals, CONFIG[\"weak_labeling\"][\"hallucination\"][\"thresholds\"])\n",
    "        \n",
    "        rows_hallu.append({\n",
    "            \"id\": ex.ex_id,\n",
    "            \"question\": ex.question,\n",
    "            \"answer\": ex.answer,\n",
    "            \"source\": ex.source,\n",
    "            \"signals\": {\n",
    "                \"factual_precision_risk\": hall_signals.factual_precision_risk,\n",
    "                \"obscurity_risk\": hall_signals.obscurity_risk,\n",
    "                \"complexity_risk\": hall_signals.complexity_risk,\n",
    "                #\"answer_deviation\": hall_signals.answer_deviation,\n",
    "            },\n",
    "            \"votes\": hall_votes,\n",
    "        })\n",
    "\n",
    "print(f\"Generated {len(rows_domain)} domain relevance examples\")\n",
    "print(f\"Generated {len(rows_hallu)} hallucination risk examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:59:13.390159Z",
     "iopub.status.busy": "2025-08-14T08:59:13.389706Z",
     "iopub.status.idle": "2025-08-14T08:59:13.401054Z",
     "shell.execute_reply": "2025-08-14T08:59:13.399423Z",
     "shell.execute_reply.started": "2025-08-14T08:59:13.390130Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Aggregate labels using Snorkel\n",
    "print(\"Aggregating domain relevance labels...\")\n",
    "dom_labelers = [\"lexicon\", \"embed\", \"bm25_ratio\", \"cross_encoder\"]\n",
    "L_dom = build_label_matrix([r[\"votes\"] for r in rows_domain], dom_labelers)\n",
    "p_dom = aggregate_probabilities(L_dom, seed=CONFIG[\"aggregation\"][\"seed\"], labeler_names=dom_labelers)\n",
    "\n",
    "for i, r in enumerate(rows_domain):\n",
    "    r[\"score\"] = round(float(p_dom[i]), 2)\n",
    "\n",
    "print(\"Aggregating hallucination risk labels...\")\n",
    "hall_labelers = [\"factual_precision\", \"obscurity\", \"complexity\"] #\"answer_dev\"\n",
    "L_h = build_label_matrix([r[\"votes\"] for r in rows_hallu], hall_labelers)\n",
    "p_h = aggregate_probabilities(L_h, seed=CONFIG[\"aggregation\"][\"seed\"])\n",
    "    \n",
    "for i, r in enumerate(rows_hallu):\n",
    "    r[\"score\"] = round(float(p_h[i]), 2)\n",
    "\n",
    "print(\"Label aggregation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:59:17.066016Z",
     "iopub.status.busy": "2025-08-14T08:59:17.065472Z",
     "iopub.status.idle": "2025-08-14T08:59:17.187610Z",
     "shell.execute_reply": "2025-08-14T08:59:17.186390Z",
     "shell.execute_reply.started": "2025-08-14T08:59:17.065981Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create DataFrames\n",
    "df_dom = pd.DataFrame(rows_domain)\n",
    "df_hall = pd.DataFrame(rows_hallu) if rows_hallu else pd.DataFrame(columns=[\"id\"])\n",
    "\n",
    "# Save datasets\n",
    "output_dir = CONFIG[\"paths\"][\"output_dir\"]\n",
    "artifacts_dir = CONFIG[\"paths\"][\"artifacts_dir\"]\n",
    "\n",
    "# Domain relevance dataset\n",
    "dom_out_parquet = os.path.join(output_dir, \"domain_relevance_dataset.parquet\")\n",
    "dom_out_json = os.path.join(output_dir, \"domain_relevance_dataset.json\")\n",
    "hall_out_parquet = os.path.join(output_dir, \"hallucination_risk_dataset.parquet\")\n",
    "hall_out_json = os.path.join(output_dir, \"hallucination_risk_dataset.json\")\n",
    "    \n",
    "df_hall.to_parquet(hall_out_parquet, index=False)\n",
    "df_hall.to_json(hall_out_json, orient=\"records\", indent=2)\n",
    "df_dom.to_parquet(dom_out_parquet, index=False)\n",
    "df_dom.to_json(dom_out_json, orient=\"records\", indent=2)\n",
    "\n",
    "print(f\"Saved domain relevance dataset to {dom_out_parquet} and {dom_out_json}\")\n",
    "print(f\"Domain relevance dataset shape: {df_dom.shape}\")\n",
    "    \n",
    "print(f\"Saved hallucination risk dataset to {hall_out_parquet} and {hall_out_json}\")\n",
    "print(f\"Hallucination risk dataset shape: {df_hall.shape}\")\n",
    "\n",
    "# Save labeler info\n",
    "with open(os.path.join(artifacts_dir, \"domain_labelers.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(dom_labelers))\n",
    "\n",
    "with open(os.path.join(artifacts_dir, \"hallucination_labelers.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(hall_labelers))\n",
    "\n",
    "print(\"\\nDataset building completed successfully!\")\n",
    "print(f\"Output files saved to: {output_dir}\")\n",
    "print(f\"Artifact files saved to: {artifacts_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Dataset Analysis and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-14T08:59:26.903022Z",
     "iopub.status.busy": "2025-08-14T08:59:26.902626Z",
     "iopub.status.idle": "2025-08-14T08:59:26.965405Z",
     "shell.execute_reply": "2025-08-14T08:59:26.963630Z",
     "shell.execute_reply.started": "2025-08-14T08:59:26.902999Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Display dataset statistics\n",
    "print(\"=== DOMAIN RELEVANCE DATASET STATISTICS ===\")\n",
    "print(f\"Total examples: {len(df_dom)}\")\n",
    "print(f\"Score distribution:\")\n",
    "print(df_dom['score'].describe())\n",
    "print(f\"\\nSource distribution:\")\n",
    "print(df_dom['source'].value_counts())\n",
    "\n",
    "if not df_hall.empty:\n",
    "    print(\"\\n=== HALLUCINATION RISK DATASET STATISTICS ===\")\n",
    "    print(f\"Total examples: {len(df_hall)}\")\n",
    "    print(f\"Score distribution:\")\n",
    "    print(df_hall['score'].describe())\n",
    "    print(f\"\\nSource distribution:\")\n",
    "    print(df_hall['source'].value_counts())\n",
    "\n",
    "# Sample examples\n",
    "print(\"\\n=== SAMPLE DOMAIN RELEVANCE EXAMPLES ===\")\n",
    "sample_domain = df_dom.sample(n=min(3, len(df_dom)), random_state=42)\n",
    "for _, row in sample_domain.iterrows():\n",
    "    print(f\"ID: {row['id']}\")\n",
    "    print(f\"Question: {row['question'][:100]}...\")\n",
    "    print(f\"Score: {row['score']}\")\n",
    "    print(f\"Source: {row['source']}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "if not df_hall.empty:\n",
    "    print(\"\\n=== SAMPLE HALLUCINATION RISK EXAMPLES ===\")\n",
    "    sample_hall = df_hall.sample(n=min(3, len(df_hall)), random_state=42)\n",
    "    for _, row in sample_hall.iterrows():\n",
    "        print(f\"ID: {row['id']}\")\n",
    "        print(f\"Question: {row['question'][:100]}...\")\n",
    "        print(f\"Answer: {str(row['answer'])[:100]}...\")\n",
    "        print(f\"Score: {row['score']}\")\n",
    "        print(f\"Source: {row['source']}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Prepare Files for Download\n",
    "\n",
    "All output files are saved to `/kaggle/working/` and will be available for download after the notebook completes."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8067651,
     "sourceId": 12762063,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
